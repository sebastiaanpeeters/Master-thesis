{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0fd7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import io\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c124739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_audio(file_path, target_sr=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "    return audio\n",
    "\n",
    "\n",
    "def create_dataset(filepaths, transcriptions):        \n",
    "    dataset_dict = {\n",
    "        'audio': filepaths,\n",
    "        'transcription': transcriptions\n",
    "    }\n",
    "    \n",
    "    return dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec50d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpuslink_names(xml_file):\n",
    "    with open(xml_file, 'r') as file:\n",
    "        xml_content = file.read()\n",
    "\n",
    "    # remove entity reference as it interferes with the parsing.\n",
    "    xml_content = xml_content.replace('&cgnSessionsPrefix;', '')\n",
    "    root = ET.fromstring(xml_content)\n",
    "    \n",
    "    # List to store the 'name' attributes\n",
    "    names = []\n",
    "\n",
    "    #Find the name attributes for each corpus link.\n",
    "    corpus = root.find('{http://www.mpi.nl/IMDI/Schema/IMDI}Corpus')\n",
    "    for corpuslink in corpus.findall('{http://www.mpi.nl/IMDI/Schema/IMDI}CorpusLink'):\n",
    "        \n",
    "        name = corpuslink.get('Name')\n",
    "        if name is not None:\n",
    "            names.append(name)\n",
    "\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0218d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_to_directory(file_list, dest_directory):\n",
    "    # Check if destination directory exists\n",
    "    if os.path.exists(dest_directory):\n",
    "        # If it exists, delete it and recreate\n",
    "        shutil.rmtree(dest_directory)\n",
    "    \n",
    "    # Create the destination directory\n",
    "    os.makedirs(dest_directory)\n",
    "    \n",
    "    print(\"Starting Copying files to new directory.\")\n",
    "    for file_path in tqdm(file_list):\n",
    "        # Get the base name of the file\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        # Construct the destination file path\n",
    "        dest_file_path = os.path.join(dest_directory, file_name)\n",
    "        \n",
    "        # Copy the file to the destination directory\n",
    "        shutil.copy2(file_path, dest_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec3ee35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_in_range(lst, v1, v2):\n",
    "    \"\"\"\n",
    "    Return subset of list lst between (but not including) the indices of the last instance of value v1 and\n",
    "    the first instance of value v2.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        idx_1 = len(lst) - 1 - lst[::-1].index(v1)\n",
    "        idx_2 = len(lst) - 1 - lst[::-1].index(v2) #lst.index(v2)\n",
    "    except:\n",
    "        print(\"Failed to find one of the indices\")\n",
    "        print(v1, v2)\n",
    "        return []\n",
    "    \n",
    "    return lst[idx_1 + 1:idx_2]\n",
    "\n",
    "def text_from_subset(subset):\n",
    "    ignore_list = ['TextGrid', 'IntervalTier', 'BACKGROUND', 'COMMENT']\n",
    "    frags = []\n",
    "    for s in subset:\n",
    "        if len(s) > 2 and s[0] == '\"':\n",
    "            if not s.replace('\"', '') in ignore_list:\n",
    "                frags.append(s.replace('\"', ''))\n",
    "    frags = ' '.join(frags)\n",
    "    return frags\n",
    "\n",
    "def annotation_to_split_transcripts(file_data, max_len=30):\n",
    "    transcripts = []\n",
    "    split_points = []\n",
    "    \n",
    "    #1. Collect all timestamps\n",
    "    timestamps = []\n",
    "    \n",
    "    lines = file_data.split('\\n')\n",
    "    lines = lines[0:lines.index('\"BACKGROUND\"')] #Retain only the actual transcript data\n",
    "    \n",
    "    for line in lines:\n",
    "        try:\n",
    "            l = float(line)\n",
    "            if len(line) > 3:\n",
    "                timestamps.append(line) \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #2. From all timestamps, select some to create ranges <30 seconds\n",
    "    #Remove duplicats and sort from small to large\n",
    "    timestamps = [x.zfill(7) for x in timestamps]\n",
    "    timestamps = sorted(list(set(timestamps)))\n",
    "    timestamps = [x.lstrip('0') for x in timestamps]\n",
    "    \n",
    "    tmp = []\n",
    "    for i in range(len(timestamps)):\n",
    "        if timestamps[i][0] == '.':\n",
    "            tmp.append('0' + timestamps[i])\n",
    "        else:\n",
    "            tmp.append(timestamps[i])\n",
    "    \n",
    "    timestamps = tmp\n",
    "    \n",
    "    #print(timestamps)\n",
    "    \n",
    "    last_point = 0\n",
    "    for i in range(len(timestamps)):\n",
    "        if float(timestamps[i]) - last_point >= max_len: #Range is too big as of this step. Backtrack one step and use that as the split point.\n",
    "            split_points.append(timestamps[i-1])\n",
    "            last_point = float(timestamps[i-1])\n",
    "    \n",
    "    #3. Extract the text for each range\n",
    "    \n",
    "    #First range:\n",
    "    subset = text_in_range(lines, timestamps[0], split_points[0])\n",
    "    transcripts.append(text_from_subset(subset))\n",
    "    #Middle ranges:\n",
    "    for i in range(len(split_points) - 1):\n",
    "        subset = text_in_range(lines, split_points[i], split_points[i+1])\n",
    "        transcripts.append(text_from_subset(subset))\n",
    "    #Final range:\n",
    "    subset = text_in_range(lines, split_points[-1], timestamps[-1])\n",
    "    transcripts.append(text_from_subset(subset))\n",
    "    \n",
    "    split_points = [float(x) for x in split_points]\n",
    "    \n",
    "    return transcripts, split_points\n",
    "\n",
    "\n",
    "def split_audio(file_path, timestamps, output_folder='audio'):\n",
    "    \"\"\"\n",
    "    Splits a wav file into multiple smaller files based on the given timestamps.\n",
    "    \n",
    "    :param file_path: Path to the input wav file.\n",
    "    :param timestamps: List of timestamps (in seconds) where the file should be split.\n",
    "    :param output_folder: Folder where the split files should be saved. \n",
    "    \"\"\"\n",
    "    # Convert timestamps to ms\n",
    "    timestamps = [x*1000 for x in timestamps]\n",
    "    \n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_wav(file_path)\n",
    "\n",
    "    # Initial and final timestamp (start of the audio and end of the audio)\n",
    "    start_time = 0\n",
    "    timestamps.append(len(audio))  # Add the end of the audio file as the final timestamp\n",
    "\n",
    "    session_name = file_path.split('/')[-1][:-4]\n",
    "    \n",
    "    file_names = []\n",
    "    \n",
    "    # Split and export audio segments\n",
    "    for i, end_time in enumerate(timestamps):\n",
    "        if start_time < end_time:\n",
    "            segment = audio[start_time:end_time]\n",
    "            f_name = f\"{output_folder}/{session_name}_{i+1}.wav\"\n",
    "            segment.export(f_name, format=\"wav\")\n",
    "            file_names.append(f_name)\n",
    "            start_time = end_time\n",
    "    \n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5807687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(corpus_file, audio_dir):\n",
    "    #Extract the session names from the subcorpus\n",
    "    names = extract_corpuslink_names(corpus_file)\n",
    "    \n",
    "    #From the subcorpus, only select those files that are in the relevant folder (e.g. read speech)\n",
    "    files = os.listdir(audio_dir)\n",
    "    rt_names = []\n",
    "    for name in names:\n",
    "        if name + \".wav\" in files:\n",
    "            rt_names.append(name)\n",
    "    \n",
    "    #Intermediate output\n",
    "    print(f'Total amount of files in the subcorpus: {len(names)}')\n",
    "    print(f'Total amount of files selected: {len(rt_names)}')\n",
    "    \n",
    "    return rt_names\n",
    "\n",
    "def check_recording_length(names):\n",
    "    #Load the recordings metadata to check the text is long enough\n",
    "    df = pd.read_csv('data/meta/text/recordings.txt', sep='\\t')\n",
    "    df_sel = df[df['recordingID'].isin(names)]\n",
    "    audio_len = df_sel.secCount.sum()/3600\n",
    "    print(f'Total found audio data: {audio_len.round(1)}h')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bc94ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_cgn(corpus_file, audio_dir, transcript_dir, dataset_name = \"my_dataset\", max_len=30):\n",
    "    #Get the session names from the subcorpus that are in the relevant folder\n",
    "    rt_names = get_names(corpus_file, audio_dir)\n",
    "    \n",
    "    #Check how many hours of audio data there are.\n",
    "    check_recording_length(rt_names)\n",
    "    \n",
    "    # List all .gz files in the transcription directory\n",
    "    gz_files = [f for f in os.listdir(transcript_dir) if f.endswith('.gz')]\n",
    "\n",
    "    #Set up audio directory\n",
    "    audio_dir_out = 'audio'\n",
    "    # Check if destination directory exists\n",
    "    if os.path.exists(audio_dir_out):\n",
    "        # If it exists, delete it and recreate\n",
    "        shutil.rmtree(audio_dir_out)\n",
    "    \n",
    "    # Create the destination directory\n",
    "    os.makedirs(audio_dir_out)\n",
    "    \n",
    "    \n",
    "    audio_names = []\n",
    "    transcript_list = []\n",
    "    \n",
    "    print('Start Chunking Files.')\n",
    "    \n",
    "    for gz_file in tqdm(gz_files):\n",
    "        if gz_file[:-7] in rt_names: #We only need the selected files. \n",
    "            gz_file_path = os.path.join(transcript_dir, gz_file)\n",
    "\n",
    "            # Open the .gz file\n",
    "            with gzip.open(gz_file_path, 'rb') as f:\n",
    "                # Read the contents of the file\n",
    "                file_content = f.read()\n",
    "                text_content = file_content.decode('ansi')\n",
    "                text_content = text_content.replace('\\r', '')\n",
    "                \n",
    "                #Split the audio file and transcript into smaller chunks\n",
    "                transcripts, split_points = annotation_to_split_transcripts(text_content, max_len=max_len)\n",
    "                split_file_names = split_audio(f'{audio_dir}/{gz_file[:-7]}.wav', split_points)\n",
    "                \n",
    "                audio_names.extend(split_file_names)\n",
    "                transcript_list.extend(transcripts)\n",
    "    \n",
    "    print('Finished chunking Files.')\n",
    "    \n",
    "    #Finally, create and return a Dataset Object.    \n",
    "    dataset = Dataset.from_dict(create_dataset(audio_names, transcript_list))\n",
    "    \n",
    "    dataset.save_to_disk(dataset_name)\n",
    "    print(\"Dataset saved to disk\")\n",
    "    \n",
    "    print(\"Zipping dataset and audio files\")\n",
    "    with zipfile.ZipFile(dataset_name + '.zip', 'w') as zipf:\n",
    "        for root, _, files in os.walk('audio'):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file))\n",
    "        for root, _, files in os.walk(dataset_name):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file))\n",
    "    \n",
    "    print(\"Removing audio folder and dataset file.\")\n",
    "    shutil.rmtree('audio')\n",
    "    shutil.rmtree(dataset_name)\n",
    "    print(f\"Dataset has been successfully created. It can be found in {dataset_name}.zip\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fe0ff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of files in the subcorpus: 3611\n",
      "Total amount of files selected: 182\n",
      "Total found audio data: 19.4h\n",
      "Start Chunking Files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 561/561 [00:31<00:00, 17.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished chunking Files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26132ce31ff47ef9816c3f217d4a364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7773 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to disk\n",
      "Zipping dataset and audio files\n",
      "Removing audio folder and dataset file.\n",
      "Dataset has been successfully created. It can be found in dataset_south_hollandic_v3.zip\n"
     ]
    }
   ],
   "source": [
    "corpus_file = 'data/meta/imdi/corpora/regN1A.imdi' #South Holland\n",
    "audio_dir = 'data/audio/wav/comp-o/nl/'\n",
    "transcript_dir = 'data/annot/text/ort/comp-o/nl'\n",
    "dataset_name = \"dataset_south_hollandic_v3\"\n",
    "\n",
    "dataset = create_dataset_from_cgn(corpus_file, audio_dir, transcript_dir, dataset_name=dataset_name, max_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d23f636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0d26473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of files in the subcorpus: 1039\n",
      "Total amount of files selected: 201\n",
      "Total found audio data: 6.6h\n",
      "Start Chunking Files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1200/1200 [00:10<00:00, 110.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished chunking Files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f4669959614b8f91f26a64d39afc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2803 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to disk\n",
      "Zipping dataset and audio files\n",
      "Removing audio folder and dataset file.\n",
      "Dataset has been successfully created. It can be found in dataset_west_flemish_v2.zip\n"
     ]
    }
   ],
   "source": [
    "corpus_file = 'data/meta/imdi/corpora/regV3.imdi' #West-Flanders\n",
    "audio_dir = 'data/audio/wav/comp-o/vl/'\n",
    "transcript_dir = 'data/annot/text/ort/comp-o/vl'\n",
    "dataset_name = \"dataset_west_flemish_v2\"\n",
    "\n",
    "dataset = create_dataset_from_cgn(corpus_file, audio_dir, transcript_dir, dataset_name=dataset_name, max_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f9cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be7ecb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of files in the subcorpus: 1179\n",
      "Total amount of files selected: 215\n",
      "Total found audio data: 6.8h\n",
      "Start Chunking Files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1200/1200 [00:11<00:00, 103.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished chunking Files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e932143ac04a7fa6bd9b9a804c9850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to disk\n",
      "Zipping dataset and audio files\n",
      "Removing audio folder and dataset file.\n",
      "Dataset has been successfully created. It can be found in dataset_east_flemish_v2.zip\n"
     ]
    }
   ],
   "source": [
    "corpus_file = 'data/meta/imdi/corpora/regV2.imdi' #East-Flanders\n",
    "audio_dir = 'data/audio/wav/comp-o/vl/'\n",
    "transcript_dir = 'data/annot/text/ort/comp-o/vl'\n",
    "dataset_name = \"dataset_east_flemish_v2\"\n",
    "\n",
    "dataset = create_dataset_from_cgn(corpus_file, audio_dir, transcript_dir, dataset_name=dataset_name, max_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aca7f669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'transcription'],\n",
       "    num_rows: 941\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c85bb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': 'audio/fv800281_2.wav',\n",
       " 'transcription': 'Muntes autoriteit en verantwoordelijkheid strekten zich uit over alle domeinen van het leven van zijn parochianen. een fanfare zonder zijn zegen was niet denkbaar. als pastoor voedde hij niet zozeer op tot een persoonlijke geloofshouding hij hield met gezag voor wat men als christen moest geloven en doen en zelf leefde hij dat ideaal voor. Peer Claes de broer van Ernest'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b31c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737f3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7136cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/annot/text/ort/comp-o/nl/fn001011.ort.gz', 'rb') as f:\n",
    "    # Read the contents of the file\n",
    "    file_content = f.read()\n",
    "    text_content = file_content.decode('ansi')\n",
    "    text_content = text_content.replace('\\r', '')\n",
    "\n",
    "    #Split the audio file and transcript into smaller chunks\n",
    "    transcripts, split_points = annotation_to_split_transcripts(text_content, max_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d26991e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yâ*v Abî Al-Mohtaram waarde vader. ik ben het Karima. op het moment dat ik je dit schrijf',\n",
       " 'zit ik veilig in Nederland en stort ik me volop in mijn zo lang begeerde studie. maak je maar niet ongerust over mij. ik kom er wel',\n",
       " 'ook al is de weg geplaveid met hindernissen. Ba maak je ook maar geen zorgen om jezelf. je kunt me vertrouwen hoewel dat niet je sterkste kant is',\n",
       " 'mij vertrouwen geven. ik zal je naam niet te schande maken. ik weet wat het rechte pad is. jouw eer',\n",
       " 'noch die van oom Mansour zullen door mij bezoedeld worden. als je lasterpraatjes ter ore komen weet dan dat het slecht bedoelde geruchten zijn.',\n",
       " 'achterklap en kwaadsprekerij zijn in de Marokkaanse gemeenschap geen zeldzaamheid dat weet jij net zo goed als ik. Ba',\n",
       " 'nu ik na die twaalf jaar gevangenschap mijn vrijheid heb gekregen wil ik je vertellen wie ik was en hoe ik geworden ben wie ik ben. weet je nog Ba',\n",
       " 'toen ik geboren werd in ons dorp Beni Touzine? dat dorpje hoog boven op een berg in de Rif waar je slechts te voet of met een ezeltje kon komen.',\n",
       " 'mama Siham en ik woonden bij oma jouw moeder terwijl jij in het buitenland werkte. jij was in Duitsland toen ik geboren werd.',\n",
       " 'voordat ik op de wereld kwam had de liefdesrelatie tussen jou en mama nog niet veel voorspoed gekend.',\n",
       " 'was het omdat jullie de stem van jullie harten hadden gevolgd om samen een toekomst op te bouwen tegen de wil en de gewoontes van jouw familie in? herinner je je nog',\n",
       " 'hoe mooi en aantrekkelijk je mama vanaf het eerste moment al vond? het was zo vertelde je me later liefde op het eerste gezicht.',\n",
       " 'je liet er geen gras over groeien. je vroeg aan je zwager om bij haar vader om haar hand te vragen. je zwager ging met je mee naar Laäzib',\n",
       " \"het stadje tien kilometer verderop waar mama woonde. in Laäzib werden jullie hartelijk ontvangen door mama's vader mijn opa. hij was een bijzondere man\",\n",
       " 'die veel van Marokko had gezien en als ambtenaar in verschillende steden had gewerkt naar Riffijnse maatstaven een kosmopolitische man.',\n",
       " 'dat blijkt ook wel uit het feit dat hij zijn dochter Yamina naar school had laten gaan. meisjes in de Rif gingen in die tijd hoogst zelden naar school in jouw familie al helemaal niet.',\n",
       " 'dat was niet alleen het lot van meisjes ook jij hebt nooit de gelegenheid gekregen om naar school te gaan. herinner je je nog Ba',\n",
       " \"hoe je zwager uiteindelijk mama's hand vroeg aan opa? dan weet je zeker ook nog... of ben je dat vergeten? hoe je eigen moeder jullie was gevolgd\",\n",
       " 'en buiten bij de deur stond te fulmineren? zoon van een... wat doe je me aan. schande. doe het niet. huw haar niet.',\n",
       " 'je trotseerde je moeders geschreeuw. je wees naar de twee bergen waar we in Beni Touzine op uitkeken en zei dat zolang die bergen aan elkaar vastzaten',\n",
       " 'zolang zouden ook jij en mama aan elkaar verbonden blijven. opa accepteerde het huwelijksaanzoek. kort daarop volgde de uitgebreide bruiloft.',\n",
       " 'het feest duurde vijf dagen en nachten. mama verliet haar ouderlijke huis in Laäzib en ging bij jullie in Beni Touzine wonen',\n",
       " 'in het huis van je moeder en oom Hamid. jij vertrok naar Duitsland om de hele familie te onderhouden het was midden jaren zestig',\n",
       " 'en als gastarbeider kon je in Duitsland snel rijk worden. veel van jouw ooms waren je voorgegaan. ze genoten thuis veel aanzien.',\n",
       " 'als pasgehuwde man had je plannen voor een welvarende toekomst en jij en mama wilden een gezin stichten. er was niet voldoende werk in Marokko',\n",
       " 'en ook jij besloot je geluk in het buitenland te beproeven.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6474de0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.634,\n",
       " 19.147,\n",
       " 29.075,\n",
       " 37.851,\n",
       " 47.825,\n",
       " 56.286,\n",
       " 64.894,\n",
       " 74.868,\n",
       " 83.579,\n",
       " 92.356,\n",
       " 101.934,\n",
       " 110.766,\n",
       " 119.801,\n",
       " 129.646,\n",
       " 139.136,\n",
       " 148.924,\n",
       " 158.566,\n",
       " 167.167,\n",
       " 175.805,\n",
       " 185.512,\n",
       " 195.449,\n",
       " 204.547,\n",
       " 213.968,\n",
       " 222.828,\n",
       " 232.07]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d85870bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.634\n",
      "9.512999999999998\n",
      "9.928\n",
      "8.776\n",
      "9.974000000000004\n",
      "8.460999999999999\n",
      "8.608000000000004\n",
      "9.97399999999999\n",
      "8.710999999999999\n",
      "8.777000000000001\n",
      "9.578000000000003\n",
      "8.832000000000008\n",
      "9.034999999999997\n",
      "9.844999999999985\n",
      "9.490000000000009\n",
      "9.788000000000011\n",
      "9.641999999999996\n",
      "8.600999999999999\n",
      "8.638000000000005\n",
      "9.706999999999994\n",
      "9.937000000000012\n",
      "9.097999999999985\n",
      "9.420999999999992\n",
      "8.860000000000014\n",
      "9.24199999999999\n",
      "-232.07\n"
     ]
    }
   ],
   "source": [
    "sp = [0] + split_points\n",
    "sp2 = split_points + [0]\n",
    "\n",
    "for i in range(len(sp)):\n",
    "    print(sp2[i] - sp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5290d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
